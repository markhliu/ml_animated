# Machine Learning, Animated
(click on the animations to see actions)
![](https://user-images.githubusercontent.com/50116107/170499945-128bf650-2085-490d-9c85-d699b80669e9.gif)

## Explain Machine Learning through Animated Steps

Words and pictures don't do machine learning justice...

Animations do!

Machine learning has proven to be a poerful tool: it has beaten the world Go champion; it's the brain behind self-driving cars. However, machine learning algorithms, to most people, are like black boxes and hard to understand. 

This free online book is here to help. It summarizes machine learning in three words: initialize, adjust, repeat. 
* Step 1: A machine learning model first assigns values to parameters (initialize);
* Step 2: It then makes predictions and compares them with the actual values; it changes the parameters in such a way so that the predictions will move closer to the actual values (adjust);
* Step 3: It repeats step 2 until the parameters converge (repeat). 

Better yet, the book will use animation to show step by step how machines learn: it shows you the initial values, the change in each step, and the final converged parameters and predictions. 

## Ch1: Create Animation
Learn how to create graphs and plots; convert them into animations; combine multiple animations into one.
<img src="https://gattonweb.uky.edu/faculty/lium/ml/pieplot.gif" />

## Ch2: Gradient Descent -- Where Magic Happens
Gradient descent guides the model on how to adjust parameters so they converge. The learning rate controls how fast to adust: too fast, the parameters never converge (as in the left of the animation; too small, it takes too long to train the model (as in the right of the animation)
<img src="https://gattonweb.uky.edu/faculty/lium/ml/largetosmall.gif" />

## Ch3: Introduction to Neural Networks
See how a neural network learns from ten pairs of values: ğ‘‹=âˆ’40, ğ‘Œ=âˆ’40;ğ‘‹=0, ğ‘Œ=32;...;ğ‘‹=100, ğ‘Œ=212. The model first assigns values to w and b in ğ‘Œ=wğ‘‹+b; it then uses gradient descent to adjust the values of w and b; finally it figures out a linear relation between ğ‘‹ and ğ‘Œ that corresponds to the relation between Celsius and Fahrenheit ğ‘Œ=1.8âˆ—ğ‘‹+32. This animation shows the value of w and b in each step:
<img src="https://gattonweb.uky.edu/faculty/lium/ml/nn.gif" />

## Ch4: Activation Functions
We need activation functions such as ReLu to approximate nonlinear relations (as in the left of the animation); or sigmoid to squash values to the range [0, 1] so it can be interprested as a probability (as in the right of the animation)
<img src="https://gattonweb.uky.edu/faculty/lium/ml/relusigmoid.gif" />

## Ch5: Binary Classification
See how a neural network with sigmoid activation learns from image-label pairs. During the course of training, the model weights gradually change and the prediction on a picture of a horse goes closer and closer to 1 (as in the left of the animation) and the prediction on a picture of a deer goes closer and closer to 0 (as in the right of the animation).
<img src="https://gattonweb.uky.edu/faculty/lium/ml/p_horse_deer.gif" />
## Ch2: Gradient Descent -- Where Magic Happens
Gradient descent guides the model on how to adjust parameters so they converge. The learning rate controls how fast to adust: too fast, the parameters never converge (as in the left of the animation; too small, it takes too long to train the model (as in the right of the animation)
<img src="https://gattonweb.uky.edu/faculty/lium/ml/largetosmall.gif" />
## Ch2: Gradient Descent -- Where Magic Happens
Gradient descent guides the model on how to adjust parameters so they converge. The learning rate controls how fast to adust: too fast, the parameters never converge (as in the left of the animation; too small, it takes too long to train the model (as in the right of the animation)
<img src="https://gattonweb.uky.edu/faculty/lium/ml/largetosmall.gif" />
## Ch2: Gradient Descent -- Where Magic Happens
Gradient descent guides the model on how to adjust parameters so they converge. The learning rate controls how fast to adust: too fast, the parameters never converge (as in the left of the animation; too small, it takes too long to train the model (as in the right of the animation)
<img src="https://gattonweb.uky.edu/faculty/lium/ml/largetosmall.gif" />

